{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 21.93426489830017,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.39",
  "scores": {
    "test": [
      {
        "accuracy": 0.6703429724277068,
        "f1": 0.6529101983570066,
        "f1_weighted": 0.6611355125498407,
        "hf_subset": "es",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.6703429724277068,
        "scores_per_experiment": [
          {
            "accuracy": 0.6859448554135844,
            "f1": 0.6650703253442898,
            "f1_weighted": 0.6808375374590983
          },
          {
            "accuracy": 0.695359784801614,
            "f1": 0.6730125992525006,
            "f1_weighted": 0.6863133318483826
          },
          {
            "accuracy": 0.6620712844653666,
            "f1": 0.6501073380142928,
            "f1_weighted": 0.6517195823295019
          },
          {
            "accuracy": 0.6782111634162744,
            "f1": 0.6520345547932778,
            "f1_weighted": 0.6696497818810366
          },
          {
            "accuracy": 0.6698049764626766,
            "f1": 0.6475162578276497,
            "f1_weighted": 0.657067486798334
          },
          {
            "accuracy": 0.6519838601210491,
            "f1": 0.6411272686259522,
            "f1_weighted": 0.6468490539913321
          },
          {
            "accuracy": 0.6674512441156691,
            "f1": 0.6501441010934843,
            "f1_weighted": 0.6581287867131467
          },
          {
            "accuracy": 0.6533288500336247,
            "f1": 0.6362029087185123,
            "f1_weighted": 0.644705544160695
          },
          {
            "accuracy": 0.6587088096839274,
            "f1": 0.6566754485149066,
            "f1_weighted": 0.6397039978536526
          },
          {
            "accuracy": 0.6805648957632818,
            "f1": 0.6572111813851994,
            "f1_weighted": 0.6763800224632268
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6733890801770782,
        "f1": 0.6380868554047734,
        "f1_weighted": 0.6620131215608797,
        "hf_subset": "es",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.6733890801770782,
        "scores_per_experiment": [
          {
            "accuracy": 0.6851942941465814,
            "f1": 0.6492114698911075,
            "f1_weighted": 0.6795688669115247
          },
          {
            "accuracy": 0.6960157402852927,
            "f1": 0.6553434005712407,
            "f1_weighted": 0.6838698305652127
          },
          {
            "accuracy": 0.6709296606000984,
            "f1": 0.633195026120348,
            "f1_weighted": 0.6613709965270368
          },
          {
            "accuracy": 0.6738809640924742,
            "f1": 0.6298318278578342,
            "f1_weighted": 0.6604105925514125
          },
          {
            "accuracy": 0.6797835710772258,
            "f1": 0.6402595069390927,
            "f1_weighted": 0.6637239515297518
          },
          {
            "accuracy": 0.6733890801770782,
            "f1": 0.6368756353314711,
            "f1_weighted": 0.6670137102499007
          },
          {
            "accuracy": 0.6522380718150517,
            "f1": 0.6221036527298678,
            "f1_weighted": 0.642322704630368
          },
          {
            "accuracy": 0.6556812592228234,
            "f1": 0.6294366928467141,
            "f1_weighted": 0.6454071253182313
          },
          {
            "accuracy": 0.6591244466305952,
            "f1": 0.6428789623061602,
            "f1_weighted": 0.638569491605358
          },
          {
            "accuracy": 0.6876537137235612,
            "f1": 0.6417323794538976,
            "f1_weighted": 0.6778739457199999
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}