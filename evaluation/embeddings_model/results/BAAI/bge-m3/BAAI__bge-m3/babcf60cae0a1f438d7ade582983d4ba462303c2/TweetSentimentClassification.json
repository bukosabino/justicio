{
  "dataset_revision": "d522bb117c32f5e0207344f69f7075fc9941168b",
  "evaluation_time": 1.3757116794586182,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.39",
  "scores": {
    "test": [
      {
        "accuracy": 0.54765625,
        "f1": 0.5330622374595151,
        "f1_weighted": 0.5324301455861884,
        "hf_subset": "spanish",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.54765625,
        "scores_per_experiment": [
          {
            "accuracy": 0.53125,
            "f1": 0.5219718092852421,
            "f1_weighted": 0.5214625771849419
          },
          {
            "accuracy": 0.578125,
            "f1": 0.5692276454953941,
            "f1_weighted": 0.5685870248417789
          },
          {
            "accuracy": 0.55078125,
            "f1": 0.5214820956943677,
            "f1_weighted": 0.520552798795375
          },
          {
            "accuracy": 0.40625,
            "f1": 0.3849672980740942,
            "f1_weighted": 0.3846176388841741
          },
          {
            "accuracy": 0.6015625,
            "f1": 0.5798869628366644,
            "f1_weighted": 0.5791264090177134
          },
          {
            "accuracy": 0.4921875,
            "f1": 0.4740495694841133,
            "f1_weighted": 0.473291563353316
          },
          {
            "accuracy": 0.60546875,
            "f1": 0.5914882108604925,
            "f1_weighted": 0.5908166960508048
          },
          {
            "accuracy": 0.5546875,
            "f1": 0.5476255423063933,
            "f1_weighted": 0.547038864006118
          },
          {
            "accuracy": 0.5546875,
            "f1": 0.5435550556032483,
            "f1_weighted": 0.542962621370001
          },
          {
            "accuracy": 0.6015625,
            "f1": 0.5963681849551414,
            "f1_weighted": 0.5958452623576604
          }
        ]
      }
    ]
  },
  "task_name": "TweetSentimentClassification"
}