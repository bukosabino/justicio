{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 7.296406269073486,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.39",
  "scores": {
    "test": [
      {
        "accuracy": 0.6322797579018158,
        "f1": 0.6344795048082853,
        "f1_weighted": 0.6338103481917446,
        "hf_subset": "es",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.6322797579018158,
        "scores_per_experiment": [
          {
            "accuracy": 0.6378614660390047,
            "f1": 0.6441211856912198,
            "f1_weighted": 0.6410955238984352
          },
          {
            "accuracy": 0.632817753866846,
            "f1": 0.6323402142412589,
            "f1_weighted": 0.6308114687408368
          },
          {
            "accuracy": 0.6012104909213181,
            "f1": 0.6089764942173317,
            "f1_weighted": 0.6015163208631067
          },
          {
            "accuracy": 0.6163416274377942,
            "f1": 0.6222382001506472,
            "f1_weighted": 0.6164670375162038
          },
          {
            "accuracy": 0.6624075319435104,
            "f1": 0.658909985352963,
            "f1_weighted": 0.6629982984107817
          },
          {
            "accuracy": 0.620712844653665,
            "f1": 0.6207595385090287,
            "f1_weighted": 0.620861805081355
          },
          {
            "accuracy": 0.6237390719569603,
            "f1": 0.6282548397469074,
            "f1_weighted": 0.6323356232139485
          },
          {
            "accuracy": 0.6200403496973773,
            "f1": 0.6251156284402956,
            "f1_weighted": 0.6220243300959895
          },
          {
            "accuracy": 0.6529926025554809,
            "f1": 0.6469805447067548,
            "f1_weighted": 0.6540026243049627
          },
          {
            "accuracy": 0.6546738399462004,
            "f1": 0.6570984170264453,
            "f1_weighted": 0.6559904497918266
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6385145105755042,
        "f1": 0.643565913167307,
        "f1_weighted": 0.639886171423558,
        "hf_subset": "es",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.6385145105755042,
        "scores_per_experiment": [
          {
            "accuracy": 0.6522380718150517,
            "f1": 0.6631096730907511,
            "f1_weighted": 0.6555299589714599
          },
          {
            "accuracy": 0.6335464830300049,
            "f1": 0.6392684091366081,
            "f1_weighted": 0.6324896555808442
          },
          {
            "accuracy": 0.6064928676832267,
            "f1": 0.6155976856542056,
            "f1_weighted": 0.6076551216375206
          },
          {
            "accuracy": 0.6246925725528775,
            "f1": 0.634981691018799,
            "f1_weighted": 0.6217053558393448
          },
          {
            "accuracy": 0.6827348745696016,
            "f1": 0.68610378464216,
            "f1_weighted": 0.684861393573177
          },
          {
            "accuracy": 0.6187899655681259,
            "f1": 0.6216811416797512,
            "f1_weighted": 0.6174504180586866
          },
          {
            "accuracy": 0.6296114117068372,
            "f1": 0.6329409613218124,
            "f1_weighted": 0.6375050812589357
          },
          {
            "accuracy": 0.6232169208066897,
            "f1": 0.6301117689217044,
            "f1_weighted": 0.6279577560113245
          },
          {
            "accuracy": 0.6561731431382194,
            "f1": 0.6509461906233898,
            "f1_weighted": 0.655488803389411
          },
          {
            "accuracy": 0.6576487948844073,
            "f1": 0.66091782558389,
            "f1_weighted": 0.6582181699148762
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}