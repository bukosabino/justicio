{
  "dataset_revision": "8a04d940a42cd40658986fdd8e3da561533a3646",
  "evaluation_time": 3.692772150039673,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.39",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.6065,
          "accuracy_threshold": 0.960740864276886,
          "ap": 0.5995508319223644,
          "f1": 0.6249555634553857,
          "f1_threshold": 0.6940346956253052,
          "precision": 0.46117523609653727,
          "recall": 0.9691289966923925
        },
        "dot": {
          "accuracy": 0.5585,
          "accuracy_threshold": 135.313232421875,
          "ap": 0.4940203240642238,
          "f1": 0.6245250431778929,
          "f1_threshold": 50.725486755371094,
          "precision": 0.45472837022132795,
          "recall": 0.9966923925027563
        },
        "euclidean": {
          "accuracy": 0.609,
          "accuracy_threshold": 2.660865306854248,
          "ap": 0.6015051251246853,
          "f1": 0.6240057845263919,
          "f1_threshold": 7.785566329956055,
          "precision": 0.464228079612695,
          "recall": 0.9514884233737596
        },
        "hf_subset": "es",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.6015684593563027,
        "manhattan": {
          "accuracy": 0.609,
          "accuracy_threshold": 58.04786682128906,
          "ap": 0.6015684593563027,
          "f1": 0.624548736462094,
          "f1_threshold": 172.89822387695312,
          "precision": 0.4643048845947397,
          "recall": 0.9536934950385888
        },
        "max": {
          "accuracy": 0.609,
          "ap": 0.6015684593563027,
          "f1": 0.6249555634553857
        },
        "similarity": {
          "accuracy": 0.6065,
          "accuracy_threshold": 0.960740864276886,
          "ap": 0.5995508319223645,
          "f1": 0.6249555634553857,
          "f1_threshold": 0.6940346360206604,
          "precision": 0.46117523609653727,
          "recall": 0.9691289966923925
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.62,
          "accuracy_threshold": 0.9686229825019836,
          "ap": 0.5675064413010118,
          "f1": 0.6046010064701653,
          "f1_threshold": 0.5832871794700623,
          "precision": 0.434625322997416,
          "recall": 0.9929161747343566
        },
        "dot": {
          "accuracy": 0.58,
          "accuracy_threshold": 161.220703125,
          "ap": 0.46692153543214066,
          "f1": 0.6065934065934067,
          "f1_threshold": 70.09696960449219,
          "precision": 0.4397238449283059,
          "recall": 0.9775678866587958
        },
        "euclidean": {
          "accuracy": 0.624,
          "accuracy_threshold": 2.9109959602355957,
          "ap": 0.5704933965685662,
          "f1": 0.6045135035146134,
          "f1_threshold": 8.24219799041748,
          "precision": 0.4401939655172414,
          "recall": 0.9645808736717828
        },
        "hf_subset": "es",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.5704933965685662,
        "manhattan": {
          "accuracy": 0.624,
          "accuracy_threshold": 63.25823211669922,
          "ap": 0.5703471366686128,
          "f1": 0.604737231680237,
          "f1_threshold": 181.4298095703125,
          "precision": 0.4404312668463612,
          "recall": 0.9645808736717828
        },
        "max": {
          "accuracy": 0.624,
          "ap": 0.5704933965685662,
          "f1": 0.6065934065934067
        },
        "similarity": {
          "accuracy": 0.62,
          "accuracy_threshold": 0.9686229825019836,
          "ap": 0.5677406947820186,
          "f1": 0.6046010064701653,
          "f1_threshold": 0.5832871794700623,
          "precision": 0.434625322997416,
          "recall": 0.9929161747343566
        }
      }
    ]
  },
  "task_name": "PawsX"
}