{
  "dataset_revision": "d522bb117c32f5e0207344f69f7075fc9941168b",
  "evaluation_time": 0.829547643661499,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.39",
  "scores": {
    "test": [
      {
        "accuracy": 0.408203125,
        "f1": 0.4058319047307838,
        "f1_weighted": 0.4057056459457991,
        "hf_subset": "spanish",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.408203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.4609375,
            "f1": 0.45288778578934014,
            "f1_weighted": 0.45280548264882786
          },
          {
            "accuracy": 0.39453125,
            "f1": 0.39248541653011726,
            "f1_weighted": 0.3924736519507439
          },
          {
            "accuracy": 0.4375,
            "f1": 0.43509787314135134,
            "f1_weighted": 0.43475696772656686
          },
          {
            "accuracy": 0.31640625,
            "f1": 0.317093349012368,
            "f1_weighted": 0.3170830836209331
          },
          {
            "accuracy": 0.421875,
            "f1": 0.4220059622402184,
            "f1_weighted": 0.4219832239935702
          },
          {
            "accuracy": 0.3203125,
            "f1": 0.32164384967696696,
            "f1_weighted": 0.32143491442268585
          },
          {
            "accuracy": 0.42578125,
            "f1": 0.415601055295463,
            "f1_weighted": 0.41513298339152493
          },
          {
            "accuracy": 0.42578125,
            "f1": 0.42581351551872054,
            "f1_weighted": 0.4258369712464528
          },
          {
            "accuracy": 0.390625,
            "f1": 0.3902731635289775,
            "f1_weighted": 0.39006588282115173
          },
          {
            "accuracy": 0.48828125,
            "f1": 0.4854170765743146,
            "f1_weighted": 0.48548329763553383
          }
        ]
      }
    ]
  },
  "task_name": "TweetSentimentClassification"
}