{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 2.118696928024292,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.39",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.5699633699633699,
          "accuracy_threshold": 0.5124573707580566,
          "ap": 0.5700439595753928,
          "f1": 0.6686868686868687,
          "f1_threshold": 0.23125973343849182,
          "precision": 0.5100154083204931,
          "recall": 0.9706744868035191
        },
        "dot": {
          "accuracy": 0.5721611721611721,
          "accuracy_threshold": 77.91481018066406,
          "ap": 0.5793703625227221,
          "f1": 0.6697530864197531,
          "f1_threshold": 34.103485107421875,
          "precision": 0.5158478605388273,
          "recall": 0.9545454545454546
        },
        "euclidean": {
          "accuracy": 0.5699633699633699,
          "accuracy_threshold": 8.91073226928711,
          "ap": 0.5654540110303955,
          "f1": 0.6673458991339787,
          "f1_threshold": 13.936699867248535,
          "precision": 0.5113192818110851,
          "recall": 0.9604105571847508
        },
        "hf_subset": "es",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.5793703625227221,
        "manhattan": {
          "accuracy": 0.5692307692307692,
          "accuracy_threshold": 197.28347778320312,
          "ap": 0.5650076642576558,
          "f1": 0.6666666666666666,
          "f1_threshold": 357.9945068359375,
          "precision": 0.5011070110701107,
          "recall": 0.9956011730205279
        },
        "max": {
          "accuracy": 0.5721611721611721,
          "ap": 0.5793703625227221,
          "f1": 0.6697530864197531
        },
        "similarity": {
          "accuracy": 0.5699633699633699,
          "accuracy_threshold": 0.5124572515487671,
          "ap": 0.5700439595753928,
          "f1": 0.6686868686868687,
          "f1_threshold": 0.23125974833965302,
          "precision": 0.5100154083204931,
          "recall": 0.9706744868035191
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.5846153846153846,
          "accuracy_threshold": 0.5614842176437378,
          "ap": 0.5884265584962705,
          "f1": 0.6730286288297337,
          "f1_threshold": 0.21053043007850647,
          "precision": 0.5118411000763942,
          "recall": 0.9824046920821115
        },
        "dot": {
          "accuracy": 0.5794871794871795,
          "accuracy_threshold": 68.42404174804688,
          "ap": 0.5759231659121954,
          "f1": 0.6742268041237113,
          "f1_threshold": 33.66102600097656,
          "precision": 0.519872813990461,
          "recall": 0.9589442815249267
        },
        "euclidean": {
          "accuracy": 0.5868131868131868,
          "accuracy_threshold": 10.251142501831055,
          "ap": 0.5884237060443422,
          "f1": 0.6744540375825291,
          "f1_threshold": 14.045129776000977,
          "precision": 0.515928515928516,
          "recall": 0.9736070381231672
        },
        "hf_subset": "es",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.5884265584962705,
        "manhattan": {
          "accuracy": 0.5846153846153846,
          "accuracy_threshold": 223.50860595703125,
          "ap": 0.5883653597378161,
          "f1": 0.6739926739926742,
          "f1_threshold": 293.31536865234375,
          "precision": 0.5240032546786005,
          "recall": 0.9442815249266863
        },
        "max": {
          "accuracy": 0.5868131868131868,
          "ap": 0.5884265584962705,
          "f1": 0.6744540375825291
        },
        "similarity": {
          "accuracy": 0.5846153846153846,
          "accuracy_threshold": 0.5614842772483826,
          "ap": 0.5884265584962705,
          "f1": 0.6730286288297337,
          "f1_threshold": 0.21053044497966766,
          "precision": 0.5118411000763942,
          "recall": 0.9824046920821115
        }
      }
    ]
  },
  "task_name": "XNLI"
}