{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 3.9763050079345703,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.39",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.7186813186813187,
          "accuracy_threshold": 0.8673545122146606,
          "ap": 0.7603625574106656,
          "f1": 0.7450980392156864,
          "f1_threshold": 0.8544580936431885,
          "precision": 0.6551724137931034,
          "recall": 0.8636363636363636
        },
        "dot": {
          "accuracy": 0.7186813186813187,
          "accuracy_threshold": 0.8673545122146606,
          "ap": 0.7603625574106655,
          "f1": 0.7450980392156864,
          "f1_threshold": 0.8544580936431885,
          "precision": 0.6551724137931034,
          "recall": 0.8636363636363636
        },
        "euclidean": {
          "accuracy": 0.7186813186813187,
          "accuracy_threshold": 0.5150640606880188,
          "ap": 0.7603625574106656,
          "f1": 0.7450980392156864,
          "f1_threshold": 0.5395218133926392,
          "precision": 0.6551724137931034,
          "recall": 0.8636363636363636
        },
        "hf_subset": "es",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.7603625574106656,
        "manhattan": {
          "accuracy": 0.7223443223443223,
          "accuracy_threshold": 13.11111068725586,
          "ap": 0.759756685283129,
          "f1": 0.7434292866082604,
          "f1_threshold": 13.756207466125488,
          "precision": 0.648471615720524,
          "recall": 0.8709677419354839
        },
        "max": {
          "accuracy": 0.7223443223443223,
          "ap": 0.7603625574106656,
          "f1": 0.7450980392156864
        },
        "similarity": {
          "accuracy": 0.7186813186813187,
          "accuracy_threshold": 0.8673545718193054,
          "ap": 0.7603625574106655,
          "f1": 0.7450980392156864,
          "f1_threshold": 0.8544581532478333,
          "precision": 0.6551724137931034,
          "recall": 0.8636363636363636
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.7304029304029304,
          "accuracy_threshold": 0.8638375997543335,
          "ap": 0.7850981749776771,
          "f1": 0.7440918298446996,
          "f1_threshold": 0.8607165217399597,
          "precision": 0.6896120150187734,
          "recall": 0.8079178885630498
        },
        "dot": {
          "accuracy": 0.7304029304029304,
          "accuracy_threshold": 0.863837718963623,
          "ap": 0.7850981749776771,
          "f1": 0.7440918298446996,
          "f1_threshold": 0.8607164621353149,
          "precision": 0.6896120150187734,
          "recall": 0.8079178885630498
        },
        "euclidean": {
          "accuracy": 0.7304029304029304,
          "accuracy_threshold": 0.5218473672866821,
          "ap": 0.7850981749776771,
          "f1": 0.7440918298446996,
          "f1_threshold": 0.5277944207191467,
          "precision": 0.6896120150187734,
          "recall": 0.8079178885630498
        },
        "hf_subset": "es",
        "languages": [
          "spa-Latn"
        ],
        "main_score": 0.7850981749776771,
        "manhattan": {
          "accuracy": 0.7296703296703296,
          "accuracy_threshold": 13.037092208862305,
          "ap": 0.7830208746989458,
          "f1": 0.7421555252387448,
          "f1_threshold": 13.392582893371582,
          "precision": 0.6938775510204082,
          "recall": 0.7976539589442815
        },
        "max": {
          "accuracy": 0.7304029304029304,
          "ap": 0.7850981749776771,
          "f1": 0.7440918298446996
        },
        "similarity": {
          "accuracy": 0.7304029304029304,
          "accuracy_threshold": 0.8638375997543335,
          "ap": 0.7850970469539559,
          "f1": 0.7440918298446996,
          "f1_threshold": 0.8607165813446045,
          "precision": 0.6896120150187734,
          "recall": 0.8079178885630498
        }
      }
    ]
  },
  "task_name": "XNLI"
}