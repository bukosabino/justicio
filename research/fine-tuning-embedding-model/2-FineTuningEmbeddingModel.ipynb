{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a939ea23-e8f6-41bb-8eea-e23c302cdbc2",
   "metadata": {},
   "source": [
    "# Fine-tuning the model using `BAAI/bge-m3` as baseline.\n",
    "\n",
    "Customize the embedding model (BAAI/bge-m3) for a specific domain (Legal) and language (Spanish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a946d9-8196-4702-89e9-adbb62917d56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers==3.0.1 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: accelerate==0.32.1 in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
      "Requirement already satisfied: transformers==4.42.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.42.3) (4.42.3)\n",
      "Requirement already satisfied: datasets==2.20.0 in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (2.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (1.5.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (0.23.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (10.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.32.1) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.32.1) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.32.1) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.32.1) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->transformers[torch]==4.42.3) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->transformers[torch]==4.42.3) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->transformers[torch]==4.42.3) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.3->transformers[torch]==4.42.3) (0.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers==3.0.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.3->transformers[torch]==4.42.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.3->transformers[torch]==4.42.3) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.3->transformers[torch]==4.42.3) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.3->transformers[torch]==4.42.3) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers==3.0.1) (12.3.101)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==3.0.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==3.0.1) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.0.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers==3.0.1) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers==3.0.1 accelerate==0.32.1 transformers[torch]==4.42.3 datasets==2.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "079a4395-695a-4e39-8510-e2a46f756ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformerModelCardData,\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.evaluation import (\n",
    "    InformationRetrievalEvaluator,\n",
    "    SequentialEvaluator,\n",
    ")\n",
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.util import cos_sim\n",
    "from transformers import set_seed\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9a93f-14a3-4bdb-9e54-f0f58c1ca7c5",
   "metadata": {},
   "source": [
    "# Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b761ff-c6f4-40d0-8342-85e8ca81c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de68f3eb-f780-492c-8933-6635dc52ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_own_seed(seed):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_own_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63584a8-2fc8-4f4e-a13e-7fb5169c5128",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56ad96b-9b78-40b7-8b91-9dee5bb8e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATASET = \"dariolopez/justicio-rag-embedding-qa-tmp-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "574523ce-7e99-4f60-a2a7-60a91787087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_MODEL = \"BAAI/bge-m3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9567a0fb-652b-41f2-82dc-1ee066d3eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_MODEL = \"dariolopez/bge-m3-es-legal-tmp-6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc0466e-f510-432b-b1d1-3c71c96614a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'num_train_epochs': 6,  # TODO\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 16,\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c887449d-cbe7-47da-8d94-ab38f45376ce",
   "metadata": {},
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9afb2b77-5b0b-460b-b727-fed676aa6455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 12 13:59:43 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  | 00000000:44:00.0 Off |                  Off |\n",
      "| 30%   39C    P8              24W / 300W |      2MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c12b99c9-48d8-4058-976c-468e9f39186d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e2c2e8-1ce5-4996-8f02-7c51a0745f41",
   "metadata": {},
   "source": [
    "# Load & Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51bda3f7-5525-4391-bf0f-c18a5a753200",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(INPUT_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65465afd-04ab-4c44-90bf-e4ac057a834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una columna 'id' autoincremental\n",
    "ids = list(range(1, len(dataset['train']) + 1))\n",
    "\n",
    "# Añadir la columna 'id' al dataset\n",
    "dataset['train'] = dataset['train'].add_column('id', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f81ad15b-0110-4754-a029-dae2c09507c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'context', 'answer', 'id'],\n",
       "        num_rows: 2947\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'context', 'answer', 'id'],\n",
       "        num_rows: 328\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 0.1\n",
    "\n",
    "dataset = dataset['train'].train_test_split(test_size=test_size)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bbc922-a3bb-4e95-afe0-c596ce424eec",
   "metadata": {},
   "source": [
    "# Model & Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6918d614-ced5-4139-b66e-3ea1c71246f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "matryoshka_dimensions=[1024, 768, 512, 256, 128, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c26aa7c-674f-47b2-8a4e-bea29ea04b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\n",
    "    INPUT_MODEL,\n",
    "    device=\"cuda\",\n",
    "    # model_kwargs={\"attn_implementation\": \"sdpa\"},  # needs Ampere GPU or newer\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"es\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=\"BGE large Legal Spanish\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9009bab-aa1d-438f-9a4b-a72906a3f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluator(\n",
    "    train_dataset, test_dataset, matryoshka_dimensions=[1024, 768, 512, 256, 128, 64]\n",
    "):\n",
    "    corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "\n",
    "    # Convert the datasets to dictionaries\n",
    "    corpus = dict(\n",
    "        zip(corpus_dataset[\"id\"], corpus_dataset[\"context\"])\n",
    "    )  # Our corpus (cid => document)\n",
    "    queries = dict(\n",
    "        zip(test_dataset[\"id\"], test_dataset[\"question\"])\n",
    "    )  # Our queries (qid => question)\n",
    "\n",
    "    # Create a mapping of relevant document (1 in our case) for each query\n",
    "    relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
    "    for q_id in queries:\n",
    "        relevant_docs[q_id] = [q_id]\n",
    "\n",
    "    matryoshka_evaluators = []\n",
    "    # Iterate over the different dimensions\n",
    "    for dim in matryoshka_dimensions:\n",
    "        ir_evaluator = InformationRetrievalEvaluator(\n",
    "            queries=queries,\n",
    "            corpus=corpus,\n",
    "            relevant_docs=relevant_docs,\n",
    "            name=f\"dim_{dim}\",\n",
    "            truncate_dim=dim,  # Truncate the embeddings to a certain dimension\n",
    "            score_functions={\"cosine\": cos_sim},\n",
    "        )\n",
    "        matryoshka_evaluators.append(ir_evaluator)\n",
    "\n",
    "    # Create a sequential evaluator\n",
    "    return SequentialEvaluator(matryoshka_evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3750f6f6-9beb-474b-a39f-a73a608c8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = create_evaluator(\n",
    "    dataset['train'], dataset['test'], matryoshka_dimensions=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21550726-1bfc-43f2-ad1e-348691d040c4",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2de2c5ca-62cd-4e6e-a721-0701734e8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Matryoshka loss function with MultipleNegativesRankingLoss\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d6ceac-c0bc-4c8d-a441-edf92a39c50c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45195254-84f4-47d8-8e44-a4447a7a0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model_save_path = os.path.join('output', now)\n",
    "\n",
    "# os.makedirs(model_save_path, exist_ok=True)\n",
    "# with open(os.path.join(model_save_path, 'train_config.json'), 'w') as file:\n",
    "#     file.write(json.dumps(CONFIG, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "030ff9b2-4582-4541-ac19-8186c23c399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=model_save_path,\n",
    "    num_train_epochs=CONFIG['num_train_epochs'],  # number of epochs\n",
    "    per_device_train_batch_size=CONFIG['per_device_train_batch_size'],  # training batch size\n",
    "    per_device_eval_batch_size=CONFIG['per_device_eval_batch_size'],  # evaluation batch size\n",
    "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],  # gradient accumulation steps\n",
    "    warmup_ratio=0.1,  # warmup ratio\n",
    "    learning_rate=CONFIG['learning_rate'],  # learning rate\n",
    "    lr_scheduler_type=\"cosine\",  # use constant learning rate scheduler\n",
    "    optim=\"adamw_torch_fused\",  # use fused adamw optimizer\n",
    "    tf32=True,  # use tf32 precision # needs Ampere GPU or newer # TODO\n",
    "    bf16=True,  # use bf16 precision\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    eval_strategy=\"epoch\",  # evaluate after each epoch\n",
    "    save_strategy=\"epoch\",  # \"no\",  # \"epoch\",  # save after each epoch\n",
    "    logging_steps=5,  # log every 10 steps\n",
    "    save_total_limit=3,  # save only the last 3 models\n",
    "    load_best_model_at_end=True,  # load the best model when training ends\n",
    "    metric_for_best_model=\"eval_dim_128_cosine_ndcg@10\",  # Optimizing for the best ndcg@10 score for the 128 dimension  # TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c921c438-fdd5-46cc-923f-fa26d3aa9a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,  # bge-bm3\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'].select_columns(\n",
    "        [\"context\", \"question\"]\n",
    "    ),\n",
    "    eval_dataset=dataset['test'].select_columns(\n",
    "        ['context', 'question']\n",
    "    ),\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768b9b9-dda7-460e-86b4-7c7841fb0651",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fdb8c69-c41f-4fca-b171-f58c22e4490c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 06:44, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dim 1024 Cosine Accuracy@1</th>\n",
       "      <th>Dim 1024 Cosine Accuracy@3</th>\n",
       "      <th>Dim 1024 Cosine Accuracy@5</th>\n",
       "      <th>Dim 1024 Cosine Accuracy@10</th>\n",
       "      <th>Dim 1024 Cosine Precision@1</th>\n",
       "      <th>Dim 1024 Cosine Precision@3</th>\n",
       "      <th>Dim 1024 Cosine Precision@5</th>\n",
       "      <th>Dim 1024 Cosine Precision@10</th>\n",
       "      <th>Dim 1024 Cosine Recall@1</th>\n",
       "      <th>Dim 1024 Cosine Recall@3</th>\n",
       "      <th>Dim 1024 Cosine Recall@5</th>\n",
       "      <th>Dim 1024 Cosine Recall@10</th>\n",
       "      <th>Dim 1024 Cosine Ndcg@10</th>\n",
       "      <th>Dim 1024 Cosine Mrr@10</th>\n",
       "      <th>Dim 1024 Cosine Map@100</th>\n",
       "      <th>Dim 768 Cosine Accuracy@1</th>\n",
       "      <th>Dim 768 Cosine Accuracy@3</th>\n",
       "      <th>Dim 768 Cosine Accuracy@5</th>\n",
       "      <th>Dim 768 Cosine Accuracy@10</th>\n",
       "      <th>Dim 768 Cosine Precision@1</th>\n",
       "      <th>Dim 768 Cosine Precision@3</th>\n",
       "      <th>Dim 768 Cosine Precision@5</th>\n",
       "      <th>Dim 768 Cosine Precision@10</th>\n",
       "      <th>Dim 768 Cosine Recall@1</th>\n",
       "      <th>Dim 768 Cosine Recall@3</th>\n",
       "      <th>Dim 768 Cosine Recall@5</th>\n",
       "      <th>Dim 768 Cosine Recall@10</th>\n",
       "      <th>Dim 768 Cosine Ndcg@10</th>\n",
       "      <th>Dim 768 Cosine Mrr@10</th>\n",
       "      <th>Dim 768 Cosine Map@100</th>\n",
       "      <th>Dim 512 Cosine Accuracy@1</th>\n",
       "      <th>Dim 512 Cosine Accuracy@3</th>\n",
       "      <th>Dim 512 Cosine Accuracy@5</th>\n",
       "      <th>Dim 512 Cosine Accuracy@10</th>\n",
       "      <th>Dim 512 Cosine Precision@1</th>\n",
       "      <th>Dim 512 Cosine Precision@3</th>\n",
       "      <th>Dim 512 Cosine Precision@5</th>\n",
       "      <th>Dim 512 Cosine Precision@10</th>\n",
       "      <th>Dim 512 Cosine Recall@1</th>\n",
       "      <th>Dim 512 Cosine Recall@3</th>\n",
       "      <th>Dim 512 Cosine Recall@5</th>\n",
       "      <th>Dim 512 Cosine Recall@10</th>\n",
       "      <th>Dim 512 Cosine Ndcg@10</th>\n",
       "      <th>Dim 512 Cosine Mrr@10</th>\n",
       "      <th>Dim 512 Cosine Map@100</th>\n",
       "      <th>Dim 256 Cosine Accuracy@1</th>\n",
       "      <th>Dim 256 Cosine Accuracy@3</th>\n",
       "      <th>Dim 256 Cosine Accuracy@5</th>\n",
       "      <th>Dim 256 Cosine Accuracy@10</th>\n",
       "      <th>Dim 256 Cosine Precision@1</th>\n",
       "      <th>Dim 256 Cosine Precision@3</th>\n",
       "      <th>Dim 256 Cosine Precision@5</th>\n",
       "      <th>Dim 256 Cosine Precision@10</th>\n",
       "      <th>Dim 256 Cosine Recall@1</th>\n",
       "      <th>Dim 256 Cosine Recall@3</th>\n",
       "      <th>Dim 256 Cosine Recall@5</th>\n",
       "      <th>Dim 256 Cosine Recall@10</th>\n",
       "      <th>Dim 256 Cosine Ndcg@10</th>\n",
       "      <th>Dim 256 Cosine Mrr@10</th>\n",
       "      <th>Dim 256 Cosine Map@100</th>\n",
       "      <th>Dim 128 Cosine Accuracy@1</th>\n",
       "      <th>Dim 128 Cosine Accuracy@3</th>\n",
       "      <th>Dim 128 Cosine Accuracy@5</th>\n",
       "      <th>Dim 128 Cosine Accuracy@10</th>\n",
       "      <th>Dim 128 Cosine Precision@1</th>\n",
       "      <th>Dim 128 Cosine Precision@3</th>\n",
       "      <th>Dim 128 Cosine Precision@5</th>\n",
       "      <th>Dim 128 Cosine Precision@10</th>\n",
       "      <th>Dim 128 Cosine Recall@1</th>\n",
       "      <th>Dim 128 Cosine Recall@3</th>\n",
       "      <th>Dim 128 Cosine Recall@5</th>\n",
       "      <th>Dim 128 Cosine Recall@10</th>\n",
       "      <th>Dim 128 Cosine Ndcg@10</th>\n",
       "      <th>Dim 128 Cosine Mrr@10</th>\n",
       "      <th>Dim 128 Cosine Map@100</th>\n",
       "      <th>Dim 64 Cosine Accuracy@1</th>\n",
       "      <th>Dim 64 Cosine Accuracy@3</th>\n",
       "      <th>Dim 64 Cosine Accuracy@5</th>\n",
       "      <th>Dim 64 Cosine Accuracy@10</th>\n",
       "      <th>Dim 64 Cosine Precision@1</th>\n",
       "      <th>Dim 64 Cosine Precision@3</th>\n",
       "      <th>Dim 64 Cosine Precision@5</th>\n",
       "      <th>Dim 64 Cosine Precision@10</th>\n",
       "      <th>Dim 64 Cosine Recall@1</th>\n",
       "      <th>Dim 64 Cosine Recall@3</th>\n",
       "      <th>Dim 64 Cosine Recall@5</th>\n",
       "      <th>Dim 64 Cosine Recall@10</th>\n",
       "      <th>Dim 64 Cosine Ndcg@10</th>\n",
       "      <th>Dim 64 Cosine Mrr@10</th>\n",
       "      <th>Dim 64 Cosine Map@100</th>\n",
       "      <th>Sequential Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.959800</td>\n",
       "      <td>0.547653</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.789634</td>\n",
       "      <td>0.844512</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.263211</td>\n",
       "      <td>0.168902</td>\n",
       "      <td>0.087805</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.789634</td>\n",
       "      <td>0.844512</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.727703</td>\n",
       "      <td>0.677843</td>\n",
       "      <td>0.683303</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.260163</td>\n",
       "      <td>0.169512</td>\n",
       "      <td>0.088415</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>0.722369</td>\n",
       "      <td>0.669048</td>\n",
       "      <td>0.674367</td>\n",
       "      <td>0.542683</td>\n",
       "      <td>0.777439</td>\n",
       "      <td>0.838415</td>\n",
       "      <td>0.887195</td>\n",
       "      <td>0.542683</td>\n",
       "      <td>0.259146</td>\n",
       "      <td>0.167683</td>\n",
       "      <td>0.088720</td>\n",
       "      <td>0.542683</td>\n",
       "      <td>0.777439</td>\n",
       "      <td>0.838415</td>\n",
       "      <td>0.887195</td>\n",
       "      <td>0.724475</td>\n",
       "      <td>0.671195</td>\n",
       "      <td>0.675803</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>0.265244</td>\n",
       "      <td>0.164024</td>\n",
       "      <td>0.088415</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>0.729455</td>\n",
       "      <td>0.679181</td>\n",
       "      <td>0.683644</td>\n",
       "      <td>0.545732</td>\n",
       "      <td>0.753049</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>0.545732</td>\n",
       "      <td>0.251016</td>\n",
       "      <td>0.158537</td>\n",
       "      <td>0.084756</td>\n",
       "      <td>0.545732</td>\n",
       "      <td>0.753049</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>0.702934</td>\n",
       "      <td>0.655904</td>\n",
       "      <td>0.661607</td>\n",
       "      <td>0.472561</td>\n",
       "      <td>0.692073</td>\n",
       "      <td>0.737805</td>\n",
       "      <td>0.814024</td>\n",
       "      <td>0.472561</td>\n",
       "      <td>0.230691</td>\n",
       "      <td>0.147561</td>\n",
       "      <td>0.081402</td>\n",
       "      <td>0.472561</td>\n",
       "      <td>0.692073</td>\n",
       "      <td>0.737805</td>\n",
       "      <td>0.814024</td>\n",
       "      <td>0.646596</td>\n",
       "      <td>0.592962</td>\n",
       "      <td>0.599376</td>\n",
       "      <td>0.599376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.385800</td>\n",
       "      <td>0.424167</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.783537</td>\n",
       "      <td>0.835366</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.261179</td>\n",
       "      <td>0.167073</td>\n",
       "      <td>0.089939</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.783537</td>\n",
       "      <td>0.835366</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.727262</td>\n",
       "      <td>0.671047</td>\n",
       "      <td>0.674756</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>0.783537</td>\n",
       "      <td>0.844512</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>0.261179</td>\n",
       "      <td>0.168902</td>\n",
       "      <td>0.089939</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>0.783537</td>\n",
       "      <td>0.844512</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.723423</td>\n",
       "      <td>0.665855</td>\n",
       "      <td>0.669687</td>\n",
       "      <td>0.533537</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.838415</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.533537</td>\n",
       "      <td>0.266260</td>\n",
       "      <td>0.167683</td>\n",
       "      <td>0.089329</td>\n",
       "      <td>0.533537</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.838415</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.725100</td>\n",
       "      <td>0.669898</td>\n",
       "      <td>0.674030</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.838415</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>0.262195</td>\n",
       "      <td>0.167683</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.838415</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.726772</td>\n",
       "      <td>0.678004</td>\n",
       "      <td>0.683274</td>\n",
       "      <td>0.521341</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.856707</td>\n",
       "      <td>0.521341</td>\n",
       "      <td>0.256098</td>\n",
       "      <td>0.164024</td>\n",
       "      <td>0.085671</td>\n",
       "      <td>0.521341</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.856707</td>\n",
       "      <td>0.700064</td>\n",
       "      <td>0.648496</td>\n",
       "      <td>0.654429</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>0.716463</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>0.238821</td>\n",
       "      <td>0.156098</td>\n",
       "      <td>0.085061</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>0.716463</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.675184</td>\n",
       "      <td>0.618683</td>\n",
       "      <td>0.623347</td>\n",
       "      <td>0.623347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.170300</td>\n",
       "      <td>0.394022</td>\n",
       "      <td>0.530488</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.530488</td>\n",
       "      <td>0.266260</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.089939</td>\n",
       "      <td>0.530488</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.727805</td>\n",
       "      <td>0.671495</td>\n",
       "      <td>0.675518</td>\n",
       "      <td>0.533537</td>\n",
       "      <td>0.801829</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>0.905488</td>\n",
       "      <td>0.533537</td>\n",
       "      <td>0.267276</td>\n",
       "      <td>0.169512</td>\n",
       "      <td>0.090549</td>\n",
       "      <td>0.533537</td>\n",
       "      <td>0.801829</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>0.905488</td>\n",
       "      <td>0.731124</td>\n",
       "      <td>0.674018</td>\n",
       "      <td>0.677587</td>\n",
       "      <td>0.539634</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.844512</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.539634</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.168902</td>\n",
       "      <td>0.089939</td>\n",
       "      <td>0.539634</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.844512</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.730987</td>\n",
       "      <td>0.675754</td>\n",
       "      <td>0.679721</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.789634</td>\n",
       "      <td>0.832317</td>\n",
       "      <td>0.881098</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.263211</td>\n",
       "      <td>0.166463</td>\n",
       "      <td>0.088110</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.789634</td>\n",
       "      <td>0.832317</td>\n",
       "      <td>0.881098</td>\n",
       "      <td>0.727623</td>\n",
       "      <td>0.677364</td>\n",
       "      <td>0.682270</td>\n",
       "      <td>0.515244</td>\n",
       "      <td>0.771341</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.871951</td>\n",
       "      <td>0.515244</td>\n",
       "      <td>0.257114</td>\n",
       "      <td>0.164024</td>\n",
       "      <td>0.087195</td>\n",
       "      <td>0.515244</td>\n",
       "      <td>0.771341</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.871951</td>\n",
       "      <td>0.702748</td>\n",
       "      <td>0.647689</td>\n",
       "      <td>0.652332</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.716463</td>\n",
       "      <td>0.774390</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.238821</td>\n",
       "      <td>0.154878</td>\n",
       "      <td>0.084146</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.716463</td>\n",
       "      <td>0.774390</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.669486</td>\n",
       "      <td>0.614018</td>\n",
       "      <td>0.619572</td>\n",
       "      <td>0.619572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.373521</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.090244</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.736904</td>\n",
       "      <td>0.682802</td>\n",
       "      <td>0.686730</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.266260</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.089939</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.732951</td>\n",
       "      <td>0.678380</td>\n",
       "      <td>0.682615</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.814024</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.271341</td>\n",
       "      <td>0.169512</td>\n",
       "      <td>0.089939</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.814024</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.735266</td>\n",
       "      <td>0.681377</td>\n",
       "      <td>0.685362</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.265244</td>\n",
       "      <td>0.165854</td>\n",
       "      <td>0.088415</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>0.731630</td>\n",
       "      <td>0.681611</td>\n",
       "      <td>0.686539</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>0.252033</td>\n",
       "      <td>0.164024</td>\n",
       "      <td>0.086890</td>\n",
       "      <td>0.527439</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.820122</td>\n",
       "      <td>0.868902</td>\n",
       "      <td>0.706632</td>\n",
       "      <td>0.653749</td>\n",
       "      <td>0.658763</td>\n",
       "      <td>0.484756</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.859756</td>\n",
       "      <td>0.484756</td>\n",
       "      <td>0.239837</td>\n",
       "      <td>0.156098</td>\n",
       "      <td>0.085976</td>\n",
       "      <td>0.484756</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.859756</td>\n",
       "      <td>0.673830</td>\n",
       "      <td>0.614289</td>\n",
       "      <td>0.618874</td>\n",
       "      <td>0.618874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.364199</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.090244</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.737197</td>\n",
       "      <td>0.683010</td>\n",
       "      <td>0.686972</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.807927</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.269309</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.090244</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.807927</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.735940</td>\n",
       "      <td>0.681302</td>\n",
       "      <td>0.685259</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.810976</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.896341</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.270325</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.089634</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.810976</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.896341</td>\n",
       "      <td>0.735770</td>\n",
       "      <td>0.682937</td>\n",
       "      <td>0.687114</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.832317</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.265244</td>\n",
       "      <td>0.166463</td>\n",
       "      <td>0.088415</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.832317</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>0.731181</td>\n",
       "      <td>0.680950</td>\n",
       "      <td>0.685772</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.765244</td>\n",
       "      <td>0.814024</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.255081</td>\n",
       "      <td>0.162805</td>\n",
       "      <td>0.086585</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.765244</td>\n",
       "      <td>0.814024</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.705001</td>\n",
       "      <td>0.652323</td>\n",
       "      <td>0.657730</td>\n",
       "      <td>0.490854</td>\n",
       "      <td>0.728659</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.859756</td>\n",
       "      <td>0.490854</td>\n",
       "      <td>0.242886</td>\n",
       "      <td>0.157317</td>\n",
       "      <td>0.085976</td>\n",
       "      <td>0.490854</td>\n",
       "      <td>0.728659</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.859756</td>\n",
       "      <td>0.676922</td>\n",
       "      <td>0.618148</td>\n",
       "      <td>0.622775</td>\n",
       "      <td>0.622775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.362644</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.844512</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.168902</td>\n",
       "      <td>0.090244</td>\n",
       "      <td>0.551829</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.844512</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.737986</td>\n",
       "      <td>0.684161</td>\n",
       "      <td>0.688087</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.090244</td>\n",
       "      <td>0.548780</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.736128</td>\n",
       "      <td>0.681556</td>\n",
       "      <td>0.685489</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>0.810976</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>0.270325</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.089329</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>0.810976</td>\n",
       "      <td>0.850610</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.736263</td>\n",
       "      <td>0.684515</td>\n",
       "      <td>0.688930</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.832317</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.265244</td>\n",
       "      <td>0.166463</td>\n",
       "      <td>0.088415</td>\n",
       "      <td>0.554878</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.832317</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>0.730738</td>\n",
       "      <td>0.680399</td>\n",
       "      <td>0.685134</td>\n",
       "      <td>0.521341</td>\n",
       "      <td>0.762195</td>\n",
       "      <td>0.814024</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.521341</td>\n",
       "      <td>0.254065</td>\n",
       "      <td>0.162805</td>\n",
       "      <td>0.086585</td>\n",
       "      <td>0.521341</td>\n",
       "      <td>0.762195</td>\n",
       "      <td>0.814024</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.702848</td>\n",
       "      <td>0.649508</td>\n",
       "      <td>0.654997</td>\n",
       "      <td>0.484756</td>\n",
       "      <td>0.725610</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.484756</td>\n",
       "      <td>0.241870</td>\n",
       "      <td>0.156098</td>\n",
       "      <td>0.085366</td>\n",
       "      <td>0.484756</td>\n",
       "      <td>0.725610</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.672942</td>\n",
       "      <td>0.614667</td>\n",
       "      <td>0.619832</td>\n",
       "      <td>0.619832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 7s, sys: 48.2 s, total: 7min 55s\n",
      "Wall time: 6min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=66, training_loss=0.3664314911672563, metrics={'train_runtime': 407.2155, 'train_samples_per_second': 43.422, 'train_steps_per_second': 0.162, 'total_flos': 0.0, 'train_loss': 0.3664314911672563, 'epoch': 5.708108108108108})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e73337d-9722-4bf3-9063-a1670bd58100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model\n",
    "trainer.save_model(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f070-4ecc-454a-a040-6b3cae2850b6",
   "metadata": {},
   "source": [
    "# Push model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d3fad96-7ee0-4f0b-b506-3e59829f5289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff3526dd-96ea-40b9-a9dc-98441413ff98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e022403c2e374e98bea6b232112234b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1127661c0264839a8093e2590d8d906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89447d875d14cf98f97f1ce5465ee31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/dariolopez/bge-m3-es-legal-tmp-6/commit/42d0a03ceecf430ecfd7f3f49843b5dadb594bf9'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to the Hugging Face Hub!\n",
    "# model_name = model_name if \"/\" not in model_name else model_name.split(\"/\")[-1]\n",
    "model.push_to_hub(OUTPUT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e23b9e5-12c6-4cdd-bfaa-b3ef21ab4119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dariolopez/bge-m3-es-legal-tmp-6'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870f793-cada-4279-8a06-eb8c0374656c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94267a4a-a2c7-4bf3-b0c8-8aef9ad4591d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
